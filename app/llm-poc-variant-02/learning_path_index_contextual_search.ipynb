{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","from dotenv import load_dotenv\n","from datetime import datetime\n","import time\n","from langchain.llms import OpenAI\n","from langchain.document_loaders import TextLoader\n","from langchain.text_splitter import CharacterTextSplitter\n","from langchain.embeddings.openai import OpenAIEmbeddings\n","from langchain.chains import RetrievalQA\n","from langchain.prompts import PromptTemplate\n","from langchain.llms import OpenAI\n","from langchain.vectorstores import FAISS\n","from langchain.prompts import PromptTemplate"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from interface import app\n","import streamlit as st\n","# Define GenerateLearningPathIndexEmbeddings class: \n","#  - Load .csv file\n","#  - Chunk text\n","#    - Chunk size = 1000 characters\n","#    - Chunk overlap = 30 characters\n","#  - Create FAISS vector store from chunked text and OpenAI embeddings\n","#  - Get FAISS vector store\n","# This class is used to generate the FAISS vector store from the .csv file.\n","class GenerateLearningPathIndexEmbeddings:\n","    def __init__(self, csv_filename):\n","        load_dotenv()  # Load .env file\n","        self.openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n","        self.data_path = os.path.join('..\\..\\data', csv_filename)\n","        self.our_custom_data = None\n","        self.openai_embeddings = None\n","        self.faiss_vectorstore = None\n","        self.load_csv_data()\n","        self.get_openai_embeddings()\n","        self.create_faiss_vectorstore_with_csv_data_and_openai_embeddings()\n","           \n","    def load_csv_data(self):\n","        # Load your dataset (e.g., CSV, JSON, etc.)\n","        print(' -- Started loading .csv file for chunking purposes.')\n","        loader = TextLoader(self.data_path)\n","        document = loader.load()\n","        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=30, separator=\"\\n\")\n","        self.our_custom_data = text_splitter.split_documents(document)\n","        print(f' -- Finished spitting (i.e. chunking) text (i.e. documents) from the .csv file (i.e. {self.data_path}).')\n","        \n","    def get_openai_embeddings(self):\n","        self.openai_embeddings = OpenAIEmbeddings(openai_api_key=self.openai_api_key, request_timeout=60)\n","        \n","    def create_faiss_vectorstore_with_csv_data_and_openai_embeddings(self):\n","        faiss_vectorstore_foldername = \"faiss_learning_path_index\"\n","        if not os.path.exists(faiss_vectorstore_foldername):\n","            print(' -- Creating a new FAISS vector store from chunked text and OpenAI embeddings.')\n","            vectorstore = FAISS.from_documents(self.our_custom_data, self.openai_embeddings)\n","            vectorstore.save_local(faiss_vectorstore_foldername)\n","            print(f' -- Saved the newly created FAISS vector store at \"{faiss_vectorstore_foldername}\".')\n","        else:\n","            print(f' -- WARNING: Found existing FAISS vector store at \"{faiss_vectorstore_foldername}\", loading from cache.')\n","            print(f' -- NOTE: Delete the FAISS vector store at \"{faiss_vectorstore_foldername}\", if you wish to regenerate it from scratch for the next run.')\n","        self.faiss_vectorstore = FAISS.load_local(\n","            \"faiss_learning_path_index\", self.openai_embeddings\n","        )\n","    def get_faiss_vector_store(self):\n","        return self.faiss_vectorstore"]},{"cell_type":"markdown","metadata":{},"source":["https://discuss.streamlit.io/t/how-to-check-if-code-is-run-inside-streamlit-and-not-e-g-ipython/23439/7"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def running_inside_streamlit():\n","    \"\"\"\n","    Function to check whether python code is run within streamlit\n","    Returns\n","    -------\n","    use_streamlit : boolean\n","        True if code is run within streamlit, else False\n","    \"\"\"\n","    try:\n","        from streamlit.runtime.scriptrunner import get_script_run_ctx\n","        if not get_script_run_ctx():\n","            use_streamlit = False\n","        else:\n","            use_streamlit = True\n","    except ModuleNotFoundError:\n","        use_streamlit = False\n","    return use_streamlit"]},{"cell_type":"markdown","metadata":{},"source":["Define GenAI class:<br>\n"," - Create prompt template<br>\n"," - Create GenAI project<br>\n"," - Get response for query<br>\n","This class is used to get the response for a query from the GenAI project.<br>\n","The GenAI project is created from the FAISS vector store."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class GenAILearningPathIndex:\n","    def __init__(self, faiss_vectorstore):\n","        load_dotenv()  # Load .env file\n","        self.openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n","        self.faiss_vectorstore = faiss_vectorstore\n","        prompt_template = \\\n","            \"\"\"\n","                Use the following template to answer the question at the end, \n","                from the Learning Path Index csv file,\n","                display top 10 results in a tablular format and it \n","                should look like this:\n","                | Learning Pathway | duration  | link | Module\n","                | --- | --- | --- | --- |\n","                | ... | ... | ... | ... |\n","                it must contain a link for each line of the result in a table,\n","                consider the duration and Module information mentioned in the question,\n","                If you don't know the answer, don't make an entry in the table,\n","                {context}\n","                Question: {question}\n","            \"\"\"\n","        PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\",\"question\"])\n","        # The chain_type_kwargs are passed to the chain_type when it is created.\n","        self.chain_type_kwargs = {\"prompt\": PROMPT}\n","        # Create the GenAI project \n","        self.llm = OpenAI(temperature=1.0, openai_api_key=self.openai_api_key)\n","    # Get response for query\n","    # The response is returned as a string.   \n","       \n","    def get_response_for(self, query: str):\n","        qa = RetrievalQA.from_chain_type(\n","            llm=self.llm, chain_type=\"stuff\", \n","            retriever=self.faiss_vectorstore.as_retriever(),\n","            chain_type_kwargs=self.chain_type_kwargs\n","        )\n","        return qa.run(query)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_formatted_time(current_time = time.time()):\n","    return datetime.utcfromtimestamp(current_time).strftime('%Y-%m-%d %H:%M:%S')"]},{"cell_type":"markdown","metadata":{},"source":["  Load the model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["@st.cache_data\n","def load_model():\n","    start_time = time.time()\n","    print(f\"\\nStarted loading custom embeddings (created from .csv file) at {get_formatted_time(start_time)}\")\n","    learningPathIndexEmbeddings = GenerateLearningPathIndexEmbeddings(\"Learning_Pathway_Index.csv\")\n","    faiss_vectorstore = learningPathIndexEmbeddings.get_faiss_vector_store()\n","    end_time = time.time()\n","    print(f\"Finished loading custom embeddings (created from .csv file) at {get_formatted_time(end_time)}\")\n","    print(f\"Custom embeddings (created from .csv file) took about {end_time - start_time} seconds to load.\")\n","    return faiss_vectorstore"]},{"cell_type":"markdown","metadata":{},"source":[" Query the model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def query_gpt_model(query: str):\n","    start_time = time.time()\n","    print(f\"\\nQuery processing start time: {get_formatted_time(start_time)}\")\n","    genAIproject = GenAILearningPathIndex(faiss_vectorstore)\n","    answer = genAIproject.get_response_for(query)\n","    end_time = time.time()\n","    print(f\"\\nQuery processing finish time: {get_formatted_time(end_time)}\")\n","    print(f\"\\nAnswer (took about {end_time - start_time} seconds)\")\n","    return answer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if __name__=='__main__':\n","    faiss_vectorstore = load_model()\n","    if running_inside_streamlit():\n","        print(\"\\nStreamlit environment detected. \\nTo run a CLI interactive version just run `python main.py` in the CLI.\\n\")\n","        query_from_stream_list = app()\n","        if query_from_stream_list:\n","            answer = query_gpt_model(query_from_stream_list)\n","            st.write(answer)\n","    else:\n","        print(\"\\nCommand-line interactive environment detected.\\n\")\n","        while True:\n","            query = input(\"\\nEnter a query: \")\n","            if query == \"exit\":\n","                break\n","            if query.strip() == \"\":\n","                continue\n","            if query:\n","                answer = query_gpt_model(query)\n","                print(\"\\n\\n> Question:\")\n","                print(query)\n","                print(answer)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":2}
